<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://oxford-omni-lab.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://oxford-omni-lab.github.io/" rel="alternate" type="text/html" /><updated>2025-10-07T14:07:27+00:00</updated><id>https://oxford-omni-lab.github.io/feed.xml</id><entry><title type="html">New paper published in Human Brain Mapping</title><link href="https://oxford-omni-lab.github.io/2025/10/01/human-brain-mapping-paper.html" rel="alternate" type="text/html" title="New paper published in Human Brain Mapping" /><published>2025-10-01T00:00:00+00:00</published><updated>2025-10-07T14:01:33+00:00</updated><id>https://oxford-omni-lab.github.io/2025/10/01/human-brain-mapping-paper</id><content type="html" xml:base="https://oxford-omni-lab.github.io/2025/10/01/human-brain-mapping-paper.html"><![CDATA[<p>Our team’s paper, <strong>“Cross‑Modality Comparison of Fetal Brain Phenotypes: Insights From Short‑Interval Second‑Trimester MRI and Ultrasound Imaging,”</strong> has been accepted by <em>Human Brain Mapping</em>.</p>

<p><strong>What we did</strong><br />
We compared volumes of eight fetal brain structures from 90 pregnancies with <strong>paired MRI and 3D ultrasound</strong> acquired mostly on the <strong>same day</strong> (mean gap 1.2 days; max 4 days). Volumes were computed using automated deep‑learning pipelines.</p>

<p><strong>Key findings</strong></p>

<ul>
  <li><strong>Strong agreement</strong> across modalities for the cerebellum, cavum septum pellucidum, thalamus, and white/deep grey matter (good–excellent ICCs and high correlations).</li>
  <li><strong>Systematic bias</strong> for intracranial volume and the cortical plate: on average, ultrasound yielded <strong>larger values</strong> than MRI for these structures.</li>
  <li><strong>Limited comparability</strong> for the ventricular system and brainstem, reflecting modality‑specific appearance and field‑of‑view differences.</li>
</ul>

<p><strong>Why it matters</strong><br />
Second‑trimester neuroimaging underpins anomaly screening. Our results show where MRI and ultrasound measurements can be pooled confidently, and where <em>modality‑aware adjustments</em> are needed for robust clinical and research use.</p>

<p><strong>Team &amp; support</strong><br />
A collaboration between the <strong>University of Oxford</strong> (Department of Computer Science), <strong>King’s College London</strong>, <strong>City, University of London</strong>, and the <strong>Athinoula A. Martinos Center / Harvard Medical School</strong>, with support from the <strong>Gates Foundation</strong>, <strong>Wellcome Trust/EPSRC iFIND</strong>, the <strong>Wellcome/EPSRC Centre for Medical Engineering</strong>, and the <strong>NIHR Clinical Research Facility</strong> at Guy’s and St Thomas’.</p>

<p><strong>Read the paper</strong>: <a href="https://doi.org/10.1002/hbm.70349">{DOI 10.1002/hbm.70349}</a>. Open Access under <strong>CC BY 4.0</strong>.</p>

<p><a href="/publications#doi%3A10.1002%2Fhbm.70349"><i class="fa-solid fa-book"></i> View citation on our publications page</a></p>]]></content><author><name>OMNI Lab</name></author><category term="paper" /><summary type="html"><![CDATA[Our team’s paper, “Cross‑Modality Comparison of Fetal Brain Phenotypes: Insights From Short‑Interval Second‑Trimester MRI and Ultrasound Imaging,” has been accepted by Human Brain Mapping.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://oxford-omni-lab.github.io/images/posts/251001_US-MRI_same-day.png" /><media:content medium="image" url="https://oxford-omni-lab.github.io/images/posts/251001_US-MRI_same-day.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Oral Presentation at MLMI Workshop at MICCAI 2025</title><link href="https://oxford-omni-lab.github.io/2025/09/18/pres-hugo-mlmi.html" rel="alternate" type="text/html" title="Oral Presentation at MLMI Workshop at MICCAI 2025" /><published>2025-09-18T00:00:00+00:00</published><updated>2025-09-18T00:00:00+00:00</updated><id>https://oxford-omni-lab.github.io/2025/09/18/pres-hugo-mlmi</id><content type="html" xml:base="https://oxford-omni-lab.github.io/2025/09/18/pres-hugo-mlmi.html"><![CDATA[<!-- excerpt start -->

<p>Pak-Hei Yeung presented his work at the MLMI Workshop during MICCAI 2024. His work uses pretrained networks on natural images to improve medical image segmentation.</p>

<!-- excerpt end -->

<p><a href="/publications#doi%3A10.48550%2FarXiv.2509.15167">View publication →</a></p>]]></content><author><name>OMNI lab</name></author><category term="paper" /><category term="oral presentation" /><summary type="html"><![CDATA[Pak-Hei Yeung presented his work at the MLMI Workshop during MICCAI 2024. His work uses pretrained networks on natural images to improve medical image segmentation]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://oxford-omni-lab.github.io/images/posts/250918_incentive.jpg" /><media:content medium="image" url="https://oxford-omni-lab.github.io/images/posts/250918_incentive.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Outreach talk for UNIQUE</title><link href="https://oxford-omni-lab.github.io/2025/07/08/unique-2025-outreach.html" rel="alternate" type="text/html" title="Outreach talk for UNIQUE" /><published>2025-07-08T00:00:00+00:00</published><updated>2025-10-07T14:01:33+00:00</updated><id>https://oxford-omni-lab.github.io/2025/07/08/unique-2025-outreach</id><content type="html" xml:base="https://oxford-omni-lab.github.io/2025/07/08/unique-2025-outreach.html"><![CDATA[]]></content><author><name>OMNI Lab</name></author><category term="outreach" /><summary type="html"><![CDATA[]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://oxford-omni-lab.github.io/images/posts/250708_Unique_Jay.jpeg" /><media:content medium="image" url="https://oxford-omni-lab.github.io/images/posts/250708_Unique_Jay.jpeg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">We celebrate the new year with two papers accepted to ISBI 2025!</title><link href="https://oxford-omni-lab.github.io/2025/01/02/isbi-2025-papers.html" rel="alternate" type="text/html" title="We celebrate the new year with two papers accepted to ISBI 2025!" /><published>2025-01-02T00:00:00+00:00</published><updated>2025-10-07T14:01:33+00:00</updated><id>https://oxford-omni-lab.github.io/2025/01/02/isbi-2025-papers</id><content type="html" xml:base="https://oxford-omni-lab.github.io/2025/01/02/isbi-2025-papers.html"><![CDATA[<p><a href="/publications#doi%3A10.1109%2FISBI60581.2025.10980994">Rapidvol: Rapid Reconstruction of 3D Ultrasound Volumes from Sensorless 2D Scans</a></p>

<p><a href="/publications#doi%3A10.1109%2FISBI60581.2025.10980676">Exploring Test Time Adaptation for Subcortical Segmentation of the Fetal Brain in 3D Ultrasound</a></p>]]></content><author><name>OMNI Lab</name></author><category term="paper" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Lab Christmas Dinner!</title><link href="https://oxford-omni-lab.github.io/2024/12/11/lab-christmas-dinner.html" rel="alternate" type="text/html" title="Lab Christmas Dinner!" /><published>2024-12-11T00:00:00+00:00</published><updated>2025-10-07T14:01:33+00:00</updated><id>https://oxford-omni-lab.github.io/2024/12/11/lab-christmas-dinner</id><content type="html" xml:base="https://oxford-omni-lab.github.io/2024/12/11/lab-christmas-dinner.html"><![CDATA[]]></content><author><name>OMNI Lab</name></author><category term="event" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">LISA 2024 Challenge Win</title><link href="https://oxford-omni-lab.github.io/2024/10/10/lisa-challenge-win.html" rel="alternate" type="text/html" title="LISA 2024 Challenge Win" /><published>2024-10-10T00:00:00+00:00</published><updated>2025-10-07T14:01:33+00:00</updated><id>https://oxford-omni-lab.github.io/2024/10/10/lisa-challenge-win</id><content type="html" xml:base="https://oxford-omni-lab.github.io/2024/10/10/lisa-challenge-win.html"><![CDATA[<p>Nicola Dinsdale and team mate Vaanathi Sundaresan win the <a href="/publications#doi%3A10.48550%2FarXiv.2410.06161">LISA 2024 Challenge</a>.</p>]]></content><author><name>OMNI Lab</name></author><category term="challenge" /><summary type="html"><![CDATA[Nicola Dinsdale and team mate Vaanathi Sundaresan win the LISA 2024 Challenge.]]></summary></entry><entry><title type="html">Jayroop MICCAI 2024 Oral Presentation</title><link href="https://oxford-omni-lab.github.io/2024/10/06/jayroop-miccai-oral.html" rel="alternate" type="text/html" title="Jayroop MICCAI 2024 Oral Presentation" /><published>2024-10-06T00:00:00+00:00</published><updated>2025-10-07T14:01:33+00:00</updated><id>https://oxford-omni-lab.github.io/2024/10/06/jayroop-miccai-oral</id><content type="html" xml:base="https://oxford-omni-lab.github.io/2024/10/06/jayroop-miccai-oral.html"><![CDATA[<p>Jayroop Ramesh has an Oral presentation at <a href="/publications#doi%3A10.1007%2F978-3-031-72378-0_39">MICCAI 2024</a>!</p>]]></content><author><name>OMNI Lab</name></author><category term="paper" /><category term="oral presentation" /><summary type="html"><![CDATA[Jayroop Ramesh has an Oral presentation at MICCAI 2024!]]></summary></entry><entry><title type="html">Maddy ISUOG and FITNG Presentations &amp;amp; Award</title><link href="https://oxford-omni-lab.github.io/2024/09/01/maddy-wyburd-presentations-award.html" rel="alternate" type="text/html" title="Maddy ISUOG and FITNG Presentations &amp;amp; Award" /><published>2024-09-01T00:00:00+00:00</published><updated>2025-10-07T14:01:33+00:00</updated><id>https://oxford-omni-lab.github.io/2024/09/01/maddy-wyburd-presentations-award</id><content type="html" xml:base="https://oxford-omni-lab.github.io/2024/09/01/maddy-wyburd-presentations-award.html"><![CDATA[<p>Maddy Wyburd has Oral presentations at <a href="https://obgyn.onlinelibrary.wiley.com/doi/full/10.1002/uog.27764">ISUOG</a> and <a href="https://obgyn.onlinelibrary.wiley.com/doi/full/10.1002/uog.28091">FITNG</a> and has been awarded the Young Investigator Award at the FITNG conference!</p>]]></content><author><name>OMNI Lab</name></author><category term="award" /><category term="oral presentation" /><summary type="html"><![CDATA[Maddy Wyburd has Oral presentations at ISUOG and FITNG and has been awarded the Young Investigator Award at the FITNG conference!]]></summary></entry><entry><title type="html">Style Transfer Paper Accepted to MLCN 2024</title><link href="https://oxford-omni-lab.github.io/2024/07/15/style-transfer-mlcn.html" rel="alternate" type="text/html" title="Style Transfer Paper Accepted to MLCN 2024" /><published>2024-07-15T00:00:00+00:00</published><updated>2025-10-07T14:01:33+00:00</updated><id>https://oxford-omni-lab.github.io/2024/07/15/style-transfer-mlcn</id><content type="html" xml:base="https://oxford-omni-lab.github.io/2024/07/15/style-transfer-mlcn.html"><![CDATA[<p>Our paper on <a href="https://www.medrxiv.org/content/10.1101/2024.08.22.24312425v1">style transfer</a> has been accepted to MLCN 2024.</p>]]></content><author><name>OMNI Lab</name></author><category term="paper" /><summary type="html"><![CDATA[Our paper on style transfer has been accepted to MLCN 2024.]]></summary></entry><entry><title type="html">Joshua Omolegan wins the Gibbs prize</title><link href="https://oxford-omni-lab.github.io/2024/06/27/joshua-gibbs-prize.html" rel="alternate" type="text/html" title="Joshua Omolegan wins the Gibbs prize" /><published>2024-06-27T00:00:00+00:00</published><updated>2025-10-07T14:01:33+00:00</updated><id>https://oxford-omni-lab.github.io/2024/06/27/joshua-gibbs-prize</id><content type="html" xml:base="https://oxford-omni-lab.github.io/2024/06/27/joshua-gibbs-prize.html"><![CDATA[<p>Joshua Omolegan wins the Gibbs prize for best Part C research project in Computer Science!</p>]]></content><author><name>OMNI Lab</name></author><category term="award" /><summary type="html"><![CDATA[Joshua Omolegan wins the Gibbs prize for best Part C research project in Computer Science!]]></summary></entry></feed>